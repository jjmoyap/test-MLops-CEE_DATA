# API de PredicciÃ³n con MLflow y FastAPI - Team 51

DocumentaciÃ³n para ConstrucciÃ³n y Uso de la Imagen Docker

---

## ğŸ“‹ DescripciÃ³n General

Este proyecto contiene una API REST desarrollada con **FastAPI** que sirve un modelo de Machine Learning para predecir calificaciones de estudiantes basÃ¡ndose en su edad y horas de estudio. El modelo estÃ¡ empaquetado en formato **MLflow** y se despliega mediante **Docker**.

## ğŸ“ Estructura del Proyecto

```
docker/
â”œâ”€â”€ Dockerfile                  # DefiniciÃ³n de la imagen Docker
â”œâ”€â”€ docker-requirements.txt     # Dependencias de Python para la API
â”œâ”€â”€ README.md                   # Este archivo
â”œâ”€â”€ api/
â”‚   â””â”€â”€ Team51_ML_API.py       # CÃ³digo fuente de la API FastAPI
â””â”€â”€ model/                      # Modelo MLflow (artefactos)
    â”œâ”€â”€ MLmodel
    â”œâ”€â”€ conda.yaml
    â”œâ”€â”€ python_env.yaml
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ registered_model_meta
```

## âš™ï¸ Requisitos Previos

- Docker instalado en su sistema (versiÃ³n 20.10 o superior)
- ConexiÃ³n a internet (para descargar dependencias)
- Puerto 8880 disponible en su mÃ¡quina host

---

## ğŸ”¨ OpciÃ³n 1: Construir la Imagen desde el Dockerfile

### Paso 1: Navegar al directorio del proyecto

```bash
cd /ruta/a/tu/proyecto/docker
```

### Paso 2: Construir la imagen Docker

```bash
docker build -t team51-api:latest .
```

**ParÃ¡metros:**
- `-t team51-api:latest` : Asigna el nombre "team51-api" y la etiqueta "latest"
- `.` : Indica que el Dockerfile estÃ¡ en el directorio actual

â±ï¸ **Tiempo estimado:** 2-5 minutos (dependiendo de tu conexiÃ³n a internet)

### Paso 3: Verificar que la imagen se creÃ³ correctamente

```bash
docker images | grep team51-api
```

DeberÃ­as ver algo como:
```
team51-api    latest    abc123def456    2 minutes ago    500MB
```

### Paso 4: Ejecutar el contenedor

```bash
docker run \
  -p 8880:8880 \
  -v ./model:/ml/model \
  -v ./api:/ml/api \
  c1544c/team51-api
```

**ParÃ¡metros:**
- `-v ./model:/model` : monta un volumen de tu equipo local al contenedor
- `-v ./api:/ml/api` : monta un volumen de tu equipo local al contenedor
- `--name team51-api-container` : Nombre del contenedor
- `-p 8880:8880` : Mapea puerto 8880 del host al 8880 del contenedor
- `team51-api:latest` : Imagen a utilizar

### Paso 5: Verificar que el contenedor estÃ¡ ejecutÃ¡ndose

```bash
docker ps
```

DeberÃ­as ver el contenedor "team51-api-container" en estado "Up"

### Paso 6: Ver los logs del contenedor

```bash
docker logs team51-api-container
```

DeberÃ­as ver:
```
Usando MODEL_URI = /ml/model
Cargando modelo desde MLflow...
âœ… Modelo cargado correctamente.
INFO: Uvicorn running on http://0.0.0.0:8880
```

---

## ğŸŒ OpciÃ³n 2: Usar la Imagen PÃºblica

### Paso 1: Descargar la imagen pÃºblica desde Docker Hub

```bash
docker pull c1544c/team51-api:latest
```

> **Nota:** La imagen pÃºblica estÃ¡ disponible en `c1544c/team51-api`

### Paso 2: Ejecutar el contenedor desde la imagen pÃºblica

```bash
docker run -d \
  --name team51-api-container \
  -p 8880:8880 \
  c1544c/team51-api:latest
```

### Paso 3: Verificar el funcionamiento

```bash
docker logs team51-api-container
```

---

## ğŸ“¤ Publicar la Imagen en Docker Hub

### Paso 1: Crear una cuenta en Docker Hub

Visita: [https://hub.docker.com/signup](https://hub.docker.com/signup)

### Paso 2: Iniciar sesiÃ³n desde la terminal

```bash
docker login
```

Ingresa tu usuario y contraseÃ±a de Docker Hub

### Paso 3: Etiquetar la imagen con tu usuario

```bash
docker tag team51-api:latest <tu-usuario>/team51-api:latest
```

**Ejemplo:**
```bash
docker tag team51-api:latest c1544c/team51-api:latest
```

### Paso 4: Subir la imagen a Docker Hub

```bash
docker push <tu-usuario>/team51-api:latest
```

â±ï¸ **Tiempo estimado:** 5-15 minutos (dependiendo de tu conexiÃ³n a internet)

### Paso 5: Verificar en Docker Hub

Visita: `https://hub.docker.com/r/<tu-usuario>/team51-api`

---

## ğŸ§ª Probar la API

### OpciÃ³n A: Desde el navegador

#### 1. Health Check
```
http://localhost:8880/health
```
**Respuesta esperada:** `{"status":"ok"}`

#### 2. Hola Mundo
```
http://localhost:8880/hola_mundo
```
**Respuesta esperada:** 
```json
{"mensaje":"Hola Mundo desde la API de Team 51 con MLflow! y FastAPI"}
```

#### 3. DocumentaciÃ³n interactiva
```
http://localhost:8880/docs
```
PodrÃ¡s probar todos los endpoints desde la interfaz **Swagger UI**

### OpciÃ³n B: Desde la terminal con curl

#### 1. Health Check
```bash
curl http://localhost:8880/health
```

#### 2. PredicciÃ³n individual
```bash
curl -X POST "http://localhost:8880/predict_one" \
  -H "Content-Type: application/json" \
  -d '{"edad": 20, "horas_estudio": 5.5}'
```

**Respuesta esperada:**
```json
{
  "input": {"edad": 20, "horas_estudio": 5.5},
  "calificacion_predicha": 75.3
}
```

#### 3. PredicciÃ³n por lotes
```bash
curl -X POST "http://localhost:8880/predict_batch" \
  -H "Content-Type: application/json" \
  -d '{
    "students": [
      {"edad": 18, "horas_estudio": 3.0},
      {"edad": 22, "horas_estudio": 7.5},
      {"edad": 19, "horas_estudio": 4.2}
    ]
  }'
```

### OpciÃ³n C: Desde Python

```python
import requests
import json

# Health check
response = requests.get("http://localhost:8880/health")
print(response.json())

# PredicciÃ³n individual
data = {"edad": 20, "horas_estudio": 5.5}
response = requests.post("http://localhost:8880/predict_one", json=data)
print(response.json())

# PredicciÃ³n por lotes
batch_data = {
    "students": [
        {"edad": 18, "horas_estudio": 3.0},
        {"edad": 22, "horas_estudio": 7.5}
    ]
}
response = requests.post("http://localhost:8880/predict_batch", json=batch_data)
print(response.json())
```

---

## ğŸ› ï¸ GestiÃ³n del Contenedor

### Detener el contenedor
```bash
docker stop team51-api-container
```

### Iniciar el contenedor detenido
```bash
docker start team51-api-container
```

### Reiniciar el contenedor
```bash
docker restart team51-api-container
```

### Ver logs en tiempo real
```bash
docker logs -f team51-api-container
```

### Ejecutar comandos dentro del contenedor
```bash
docker exec -it team51-api-container /bin/bash
```

### Eliminar el contenedor
```bash
docker rm -f team51-api-container
```

### Eliminar la imagen
```bash
docker rmi team51-api:latest
```

### Ver estadÃ­sticas de uso del contenedor
```bash
docker stats team51-api-container
```

---

## ğŸ”§ Variables de Entorno Configurables

Al ejecutar el contenedor, puedes sobrescribir las variables de entorno:

```bash
docker run -d \
  --name team51-api-container \
  -p 8880:8880 \
  -e PORT=9000 \
  -e MLFLOW_TRACKING_URI=http://mi-servidor-mlflow:5000 \
  team51-api:latest
```

### Variables disponibles:

| Variable | DescripciÃ³n | Default |
|----------|-------------|---------|
| `PORT` | Puerto donde se ejecuta la API | `8880` |
| `MLFLOW_TRACKING_URI` | URI del servidor MLflow | `http://host.docker.internal:8080` |
| `MODEL_URI` | URI del modelo MLflow | `models:/student_grade_regressor/Production` |
| `MODEL_PATH` | Ruta del modelo en el contenedor | `model/` |
| `API_PATH` | Ruta de la API en el contenedor | `api/` |

---

## ğŸ’» VolÃºmenes para Desarrollo

Si deseas modificar el cÃ³digo sin reconstruir la imagen, puedes montar volÃºmenes:

```bash
docker run -d \
  --name team51-api-container \
  -p 8880:8880 \
  -v $(pwd)/api:/ml/api \
  -v $(pwd)/model:/ml/model \
  team51-api:latest
```

Esto permite editar el cÃ³digo en tiempo real y reiniciar el contenedor para aplicar los cambios sin necesidad de reconstruir la imagen.

---

## ğŸ› SoluciÃ³n de Problemas

### Problema: El contenedor no inicia

**SoluciÃ³n 1:** Verificar logs
```bash
docker logs team51-api-container
```

**SoluciÃ³n 2:** Verificar que el puerto 8880 no estÃ© en uso
```bash
# macOS/Linux
lsof -i :8880

# Windows
netstat -ano | findstr :8880
```

**SoluciÃ³n 3:** Verificar que los archivos del modelo existen
```bash
ls -la model/
```

### Problema: Error "RESOURCE_DOES_NOT_EXIST: Run with id=model not found"

**SoluciÃ³n:** Verificar que `MODEL_URI` apunte a la ruta correcta del modelo

En `Team51_ML_API.py`, asegÃºrate de que:
```python
MODEL_URI = "/ml/model"
```

### Problema: La API responde lento

**SoluciÃ³n:** Asignar mÃ¡s recursos al contenedor Docker
```bash
docker run -d \
  --name team51-api-container \
  --memory="2g" \
  --cpus="2.0" \
  -p 8880:8880 \
  team51-api:latest
```

### Problema: No puedo conectarme a la API desde fuera del host

**SoluciÃ³n:** Verificar que el puerto estÃ¡ mapeado correctamente
```bash
docker ps  # Verifica que aparezca 0.0.0.0:8880->8880/tcp
```

### Problema: Error al instalar dependencias durante la construcciÃ³n

**SoluciÃ³n 1:** Limpiar la cachÃ© de Docker
```bash
docker builder prune
```

**SoluciÃ³n 2:** Construir sin cachÃ©
```bash
docker build --no-cache -t team51-api:latest .
```

---

## ğŸ—ï¸ Arquitectura del Proyecto

### Flujo de ejecuciÃ³n:

1. Dockerfile copia el modelo y el cÃ³digo de la API al contenedor
2. Se instalan las dependencias de Python
3. Se crea un usuario no-root para seguridad
4. Uvicorn inicia el servidor FastAPI en el puerto 8880
5. MLflow carga el modelo desde `/ml/model`
6. La API queda lista para recibir peticiones

### Endpoints disponibles:

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| `GET` | `/health` | VerificaciÃ³n del estado del servicio |
| `GET` | `/hola_mundo` | Mensaje de bienvenida |
| `GET` | `/docs` | DocumentaciÃ³n interactiva Swagger |
| `POST` | `/predict_one` | PredicciÃ³n para un estudiante |
| `POST` | `/predict_batch` | PredicciÃ³n para mÃºltiples estudiantes |

### Modelo de datos:

**Input:**
```json
{"edad": int, "horas_estudio": float}
```

**Output:**
```json
{
  "input": {...},
  "calificacion_predicha": float
}
```

---

## âœ¨ Mejores PrÃ¡cticas

### 1. Seguridad:
- âœ… La imagen ejecuta el servicio con usuario no-root (`team51`)
- âœ… No incluir credenciales en el Dockerfile
- âœ… Usar variables de entorno para configuraciÃ³n sensible

### 2. OptimizaciÃ³n:
- âœ… Usar imÃ¡genes base slim para reducir el tamaÃ±o
- âœ… Multi-stage builds si el proyecto crece
- âœ… Aprovechar la cachÃ© de Docker organizando comandos correctamente

### 3. Monitoreo:
- âœ… Implementar health checks
- âœ… Centralizar logs
- âœ… Usar herramientas de monitoreo (Prometheus, Grafana)

### 4. CI/CD:
- âœ… Automatizar construcciÃ³n de imÃ¡genes
- âœ… Versionar las imÃ¡genes (tags semÃ¡nticos)
- âœ… Ejecutar pruebas antes de publicar

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n oficial:
- [Docker](https://docs.docker.com/)
- [FastAPI](https://fastapi.tiangolo.com/)
- [MLflow](https://www.mlflow.org/docs/latest/index.html)
- [Uvicorn](https://www.uvicorn.org/)

### Tutoriales recomendados:
- [Docker para principiantes](https://docker-curriculum.com/)
- [FastAPI tutorial](https://fastapi.tiangolo.com/tutorial/)
- [MLflow tracking](https://www.mlflow.org/docs/latest/tracking.html)

---

## ğŸ‘¥ Contacto y Soporte

- **Equipo:** Team 51
- **Proyecto:** MLOps - Sistema de PredicciÃ³n de Calificaciones
- **Repositorio:** test-MLops-CEE_DATA
- **Imagen Docker Hub:** [c1544c/team51-api](https://hub.docker.com/r/c1544c/team51-api)

Para reportar problemas o sugerencias, por favor crear un issue en el repositorio.

---

## ğŸ“ Changelog

### VersiÃ³n 1.0.0 (14 de noviembre de 2025)

- âœ¨ ImplementaciÃ³n inicial de la API con FastAPI
- ğŸ”— IntegraciÃ³n con MLflow para carga de modelos
- ğŸ³ DockerizaciÃ³n del servicio
- ğŸ“Š Endpoints de predicciÃ³n individual y por lotes
- ğŸ“– DocumentaciÃ³n completa

---

## ğŸ“„ Licencia

Este proyecto es parte del curso de MLOps - Team 51

---

**Â¡Gracias por usar nuestra API! ğŸš€**
